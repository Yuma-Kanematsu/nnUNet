{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yuma/Yuma-Kanematsu/nnUNet/src/raw_data\n",
      "/Users/yuma/Yuma-Kanematsu/nnUNet/src/preprocessed_data\n",
      "/Users/yuma/Yuma-Kanematsu/nnUNet/src/results\n"
     ]
    }
   ],
   "source": [
    "!echo $nnUNet_raw_data_base\n",
    "!echo $nnUNet_preprocessed\n",
    "!echo $RESULTS_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fingerprint extraction...\n",
      "Dataset007_Orbital_Trap_All_ROI\n",
      "Using <class 'nnunetv2.imageio.natural_image_reader_writer.NaturalImage2DIO'> as reader/writer\n",
      "\n",
      "####################\n",
      "verify_dataset_integrity Done. \n",
      "If you didn't see any error messages then your dataset is most likely OK!\n",
      "####################\n",
      "\n",
      "Using <class 'nnunetv2.imageio.natural_image_reader_writer.NaturalImage2DIO'> as reader/writer\n",
      "100%|████████████████████████████████████████| 517/517 [00:03<00:00, 166.38it/s]\n",
      "Experiment planning...\n",
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default planner. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "2D U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 16, 'patch_size': (np.int64(384), np.int64(512)), 'median_image_size_in_voxels': array([365., 512.]), 'spacing': array([1., 1.]), 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': (32, 64, 128, 256, 512, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
      "\n",
      "Using <class 'nnunetv2.imageio.natural_image_reader_writer.NaturalImage2DIO'> as reader/writer\n",
      "Plans were saved to /Users/yuma/Yuma-Kanematsu/nnUNet/src/preprocessed_data/Dataset007_Orbital_Trap_All_ROI/nnUNetPlans.json\n",
      "Preprocessing...\n",
      "Preprocessing dataset Dataset007_Orbital_Trap_All_ROI\n",
      "Configuration: 2d...\n",
      "100%|█████████████████████████████████████████| 517/517 [00:53<00:00,  9.75it/s]\n",
      "Configuration: 3d_fullres...\n",
      "INFO: Configuration 3d_fullres not found in plans file nnUNetPlans.json of dataset Dataset007_Orbital_Trap_All_ROI. Skipping.\n",
      "Configuration: 3d_lowres...\n",
      "INFO: Configuration 3d_lowres not found in plans file nnUNetPlans.json of dataset Dataset007_Orbital_Trap_All_ROI. Skipping.\n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_plan_and_preprocess -d 007 --verify_dataset_integrity -np 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: mps\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-04-23 16:17:51.360578: do_dummy_2d_data_aug: False\n",
      "2025-04-23 16:17:51.362025: Creating new 5-fold cross-validation split...\n",
      "2025-04-23 16:17:51.364510: Desired fold for training: 0\n",
      "2025-04-23 16:17:51.364564: This split has 399 training and 118 validation cases.\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 16, 'patch_size': [384, 512], 'median_image_size_in_voxels': [365.0, 512.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset007_Orbital_Trap_All_ROI', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [999.0, 1.0, 1.0], 'original_median_shape_after_transp': [1, 365, 512], 'image_reader_writer': 'NaturalImage2DIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 255.0, 'mean': 80.00025177001953, 'median': 81.0, 'min': 0.0, 'percentile_00_5': 39.0, 'percentile_99_5': 121.0, 'std': 14.195022583007812}}} \n",
      "\n",
      "2025-04-23 16:17:58.540097: Unable to plot network architecture:\n",
      "2025-04-23 16:17:58.540251: No module named 'hiddenlayer'\n",
      "2025-04-23 16:17:58.554948: \n",
      "2025-04-23 16:17:58.555273: Epoch 0\n",
      "2025-04-23 16:17:58.555630: Current learning rate: 0.01\n",
      "^C\n",
      "Exception ignored in atexit callbackException ignored in atexit callbackException ignored in atexit callbackException ignored in atexit callbackException ignored in atexit callbackException ignored in atexit callbackException ignored in atexit callbackException ignored in atexit callbackException ignored in atexit callbackException ignored in atexit callbackException ignored in atexit callbackException ignored in atexit callbackException ignored in atexit callbackException ignored in atexit callback: <function dump_compile_times at 0x11526f880>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      ": <function dump_compile_times at 0x113a6f880>: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      ": <function dump_compile_times at 0x10d66f880><function dump_compile_times at 0x11646f880>\n",
      "Traceback (most recent call last):\n",
      ":   File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python<function dump_compile_times at 0x11b96f880>3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      ": \n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      "<function dump_compile_times at 0x121f73880>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      "Exception ignored in atexit callback: <function dump_compile_times at 0x120173880>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      ": <function dump_compile_times at 0x10ed73880>: \n",
      ": Traceback (most recent call last):\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      ": : <function dump_compile_times at 0x111673880>: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      "<function dump_compile_times at 0x120b73880>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      "<function dump_compile_times at 0x123773880>Exception ignored in atexit callback\n",
      "Traceback (most recent call last):\n",
      ":   File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      "<function dump_compile_times at 0x10f06f880><function dump_compile_times at 0x10f373880>: Traceback (most recent call last):\n",
      "<function dump_compile_times at 0x10c76f880>  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      "\n",
      "<funcTraceback (most recent call last):\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      "tion dump_compile_times at 0x123073880>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      ": \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      "<function dump_compile_times at 0x116073880>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
      "                log.info(compile_times(repr=\"str\", aggregate=True))\n",
      "log.info(compile_times(repr=\"str\", aggregate=True))log.info(compile_times(repr=\"str\", aggregate=True))  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages    log.info(compile_times(repr=\"str\", aggregate=True))    \n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "log.info(compile_times(repr=\"str\", aggregate=True))\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "\n",
      "\n",
      "      File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "log.info(compile_times(repr=\"str\", aggregate=True))\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/pyth        log.info(compile_times(repr=\"str\", aggregate=True))log.info(compile_times(repr=\"str\", aggregate=True))\n",
      "\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "on3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "        log.info(compile_times(repr=\"str\", aggregate=True))log.info(compile_times(repr=\"str\", aggregate=True))\n",
      "      File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "log.info(compile_times(repr=\"str\", aggregate=True))\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "    out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch    out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "    out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "/_dynamo/utils.py\", line 207, in tabulate\n",
      "        import tabulateimport tabulate\n",
      "\n",
      "      File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "      File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "    out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "log.info(compile_times(repr=\"str\", aggregate=True))\n",
      "      File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/    out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "    out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "log.info(compile_times(repr=\"str\", aggregate=True))    log.info(compile_times(repr=\"str\", aggregate=True))    \n",
      "\n",
      "log.info(compile_times(repr=\"str\", aggregate=True))  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "        import tabulate\n",
      "\n",
      "      File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/tabulate/__init__.py\", line 15, in <module>\n",
      "out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/tabulate/__init__.py\", line 15, in <module>\n",
      "    import wcwidth  # optional wide-character (CJK) support\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      ".10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "\n",
      "    \n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "        import tabulateout += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/tabulate/__init__.py\", line 15, in <module>\n",
      "    out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
      "    import wcwidth  # optional wide-character (CJK) support\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages    import tabulate\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/tabulate/__init__.py\", line 15, in <module>\n",
      "/wcwidth/__init__.py\", line 12, in <module>\n",
      "        import tabulateimport tabulate\n",
      "\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/tabulate/__init__.py\", line 15, in <module>\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/tabulate/__init__.py\", line 15, in <module>\n",
      "    import wcwidth  # optional wide-character (CJK) support\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/wcwidth/__init__.py\", line 12, in <module>\n",
      "    import tabulate\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/tabulate/__init__.py\", line 15, in <module>\n",
      "    import tabulate\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/tabulate/__init__.py\", line 15, in <module>\n",
      "            import wcwidth  # optional wide-character (CJK) supportimport wcwidth  # optional wide-character (CJK) support\n",
      "\n",
      "import tabulateKeyboardInterrupt\n",
      "  File \"<frozen importlib._bootstrap>\", line 1022, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "log.info(compile_times(repr=\"str\", aggregate=True))      File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/tabulate/__init__.py\", line 15, in <module>\n",
      "\n",
      "import tabulate\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "KeyboardInterrupt:         out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "import tabulate  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dy        namo/utils.py\", line 207, in tabulate\n",
      ":     out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))    \n",
      "import wcwidth  # optional wide-character (CJK) support\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "import wcwidth  # optional wide-character (CJK) support\n",
      "    out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/wcwidth/__init__.py\", line 12, in <module>\n",
      "\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/tabulate/__init__.py\", line 15, in <module>\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/wcwidth/__init__.py\", line 12, in <module>\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "    import tabulate\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "    import wcwidth  # optional wide-character (CJK) support\n",
      "  File \"<frozen importlib._bootstrap>\", line 1024, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "      File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "    import wcwidth  # optional wide-character (CJK) support        import tabulateimport tabulate\n",
      "          File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/tabulate/__init__.py\", line 15, in <module>\n",
      "from .wcwidth import ZERO_WIDTH  # noqafrom .wcwidth import ZERO_WIDTH  # noqa\n",
      "\n",
      "  File \"<frozen importlib._bootstrap>\", line 170, in __enter__\n",
      "    import tabulate\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/tabulate/__init__.py\", line 15, in <module>\n",
      "\n",
      "  File \"<frozen importlib._bootstrap>\", line 196, in _get_module_lock\n",
      "  File \"<frozen importlib._bootstrap>\", line 73, in __init__\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "KeyboardInterrupt:     import wcwidth  # optional wide-character (CJK) support\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/wcwidth/__init__.py\", line 12, in <module>\n",
      "\n",
      "    import wcwidth  # optional wide-character (CJK) support\n",
      "  File \"<frozen importlib._bootstrap>\", line 1024, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "    from .wcwidth import ZERO_WIDTH  # noqa\n",
      "  File \"<frozen impor  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n",
      "tlib._bootstrap>\", line 1027, in _find_and_load\n",
      "    \n",
      "from .wcwidth import ZERO_WIDTH  # noqaimport wcwidth  # optional wide-character (CJK) support\n",
      "\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"/Users/yuma/Yuma-Kanematsu/nnUNet/.venv/lib/python3.10/site-packages/tabulate/__init__.py\", line 15, in <module>\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "    from .wcwidth import ZERO_WIDTH  # noqa\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 945, in _find_spec\n",
      "    import wcwidth  # optional wide-character (CJK) support\n",
      "  Fi  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1439, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1411, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 975, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1548, in find_spec\n",
      "le \"<frozen importlib._bootstrap>\", line 1024, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 170, in __enter__\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 196, in _get_module_lock\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1591, in _fill_cache\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1074, in get_data\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 975, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
      "KeyboardInterrupt: \n",
      "\n",
      "  File \"<frozen importlib._bootstrap>\", line 73, in __init__\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 670, in _compile_bytecode\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1074, in get_data\n",
      "KeyboardInterrupt: \n",
      "KeyboardInterrupt: KeyboardInterrupt: \n",
      "\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 975, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1074, in get_data\n",
      "\n",
      "KeyboardInterrupt: \n",
      "KeyboardInterrupt: \n",
      "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 170, in __enter__\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "KeyboardInterrupt  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      ": \n",
      "  File \"<frozen importlib._bootstrap_external>\", line 969, in get_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 196, in _get_module_lock\n",
      "  File \"<frozen importl  File \"<frozen importlib._bootstrap>\", line 72, in __init__\n",
      "KeyboardInterrupt: \n",
      "ib._bootstrap_external>\", line 1012, in get_code\n",
      "KeyboardInterrupt: KeyboardInterruptKeyboardInterrupt: \n",
      ":   File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "\n",
      "\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
      "KeyboardInterrupt: \n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 975, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1073, in get_data\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_train 007 2d 0 -device mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: mps\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-04-21 21:44:01.390611: do_dummy_2d_data_aug: False\n",
      "2025-04-21 21:44:01.393975: Using splits from existing split file: /Users/yuma/Yuma-Kanematsu/nnUNet/src/preprocessed_data/Dataset005_Orbital_ROI/splits_final.json\n",
      "2025-04-21 21:44:01.395162: The split file contains 5 splits.\n",
      "2025-04-21 21:44:01.395253: Desired fold for training: 0\n",
      "2025-04-21 21:44:01.395301: This split has 165 training and 40 validation cases.\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [384, 512], 'median_image_size_in_voxels': [357.0, 512.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset005_Orbital_ROI', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [999.0, 1.0, 1.0], 'original_median_shape_after_transp': [1, 357, 512], 'image_reader_writer': 'NaturalImage2DIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 255.0, 'mean': 78.51065063476562, 'median': 80.0, 'min': 0.0, 'percentile_00_5': 35.0, 'percentile_99_5': 127.0, 'std': 15.718173027038574}}} \n",
      "\n",
      "2025-04-21 21:44:08.484901: Unable to plot network architecture:\n",
      "2025-04-21 21:44:08.485015: No module named 'hiddenlayer'\n",
      "2025-04-21 21:44:08.507349: \n",
      "2025-04-21 21:44:08.507623: Epoch 0\n",
      "2025-04-21 21:44:08.511480: Current learning rate: 0.01\n",
      "2025-04-21 21:50:44.701080: train_loss -0.0007\n",
      "2025-04-21 21:50:44.706430: val_loss -0.5267\n",
      "2025-04-21 21:50:44.706556: Pseudo dice [np.float32(0.688)]\n",
      "2025-04-21 21:50:44.707020: Epoch time: 396.21 s\n",
      "2025-04-21 21:50:44.707229: Yayy! New best EMA pseudo Dice: 0.6880000233650208\n",
      "2025-04-21 21:50:46.536902: \n",
      "2025-04-21 21:50:46.537192: Epoch 1\n",
      "2025-04-21 21:50:46.537288: Current learning rate: 0.00991\n",
      "2025-04-21 21:57:15.911882: train_loss -0.652\n",
      "2025-04-21 21:57:15.915861: val_loss -0.6861\n",
      "2025-04-21 21:57:15.915981: Pseudo dice [np.float32(0.7797)]\n",
      "2025-04-21 21:57:15.916838: Epoch time: 389.38 s\n",
      "2025-04-21 21:57:15.916976: Yayy! New best EMA pseudo Dice: 0.6970999836921692\n",
      "2025-04-21 21:57:17.117681: \n",
      "2025-04-21 21:57:17.117924: Epoch 2\n",
      "2025-04-21 21:57:17.118191: Current learning rate: 0.00982\n",
      "2025-04-21 22:03:47.970287: train_loss -0.7472\n",
      "2025-04-21 22:03:47.973774: val_loss -0.713\n",
      "2025-04-21 22:03:47.973878: Pseudo dice [np.float32(0.7992)]\n",
      "2025-04-21 22:03:47.974282: Epoch time: 390.85 s\n",
      "2025-04-21 22:03:47.974367: Yayy! New best EMA pseudo Dice: 0.7074000239372253\n",
      "2025-04-21 22:03:49.370936: \n",
      "2025-04-21 22:03:49.371235: Epoch 3\n",
      "2025-04-21 22:03:49.371320: Current learning rate: 0.00973\n",
      "2025-04-21 22:10:17.923356: train_loss -0.7754\n",
      "2025-04-21 22:10:17.926560: val_loss -0.7207\n",
      "2025-04-21 22:10:17.926697: Pseudo dice [np.float32(0.8021)]\n",
      "2025-04-21 22:10:17.927321: Epoch time: 388.55 s\n",
      "2025-04-21 22:10:17.927423: Yayy! New best EMA pseudo Dice: 0.7167999744415283\n",
      "2025-04-21 22:10:19.193079: \n",
      "2025-04-21 22:10:19.193344: Epoch 4\n",
      "2025-04-21 22:10:19.193424: Current learning rate: 0.00964\n",
      "2025-04-21 22:16:40.798999: train_loss -0.7891\n",
      "2025-04-21 22:16:40.803540: val_loss -0.73\n",
      "2025-04-21 22:16:40.803634: Pseudo dice [np.float32(0.8131)]\n",
      "2025-04-21 22:16:40.804262: Epoch time: 381.61 s\n",
      "2025-04-21 22:16:40.804382: Yayy! New best EMA pseudo Dice: 0.7264000177383423\n",
      "2025-04-21 22:16:42.259673: \n",
      "2025-04-21 22:16:42.260934: Epoch 5\n",
      "2025-04-21 22:16:42.261023: Current learning rate: 0.00955\n",
      "2025-04-21 22:23:14.506519: train_loss -0.8042\n",
      "2025-04-21 22:23:14.510398: val_loss -0.7417\n",
      "2025-04-21 22:23:14.510524: Pseudo dice [np.float32(0.8187)]\n",
      "2025-04-21 22:23:14.510750: Epoch time: 392.25 s\n",
      "2025-04-21 22:23:14.510826: Yayy! New best EMA pseudo Dice: 0.7357000112533569\n",
      "2025-04-21 22:23:15.629524: \n",
      "2025-04-21 22:23:15.629850: Epoch 6\n",
      "2025-04-21 22:23:15.629964: Current learning rate: 0.00946\n",
      "2025-04-21 22:29:32.087082: train_loss -0.8189\n",
      "2025-04-21 22:29:32.090011: val_loss -0.7227\n",
      "2025-04-21 22:29:32.090114: Pseudo dice [np.float32(0.8068)]\n",
      "2025-04-21 22:29:32.090356: Epoch time: 376.46 s\n",
      "2025-04-21 22:29:32.090424: Yayy! New best EMA pseudo Dice: 0.7427999973297119\n",
      "2025-04-21 22:29:33.332356: \n",
      "2025-04-21 22:29:33.332643: Epoch 7\n",
      "2025-04-21 22:29:33.332742: Current learning rate: 0.00937\n",
      "2025-04-21 22:35:50.562347: train_loss -0.8246\n",
      "2025-04-21 22:35:50.566186: val_loss -0.7222\n",
      "2025-04-21 22:35:50.566289: Pseudo dice [np.float32(0.8093)]\n",
      "2025-04-21 22:35:50.567028: Epoch time: 377.23 s\n",
      "2025-04-21 22:35:50.567266: Yayy! New best EMA pseudo Dice: 0.7494000196456909\n",
      "2025-04-21 22:35:53.725696: \n",
      "2025-04-21 22:35:53.726962: Epoch 8\n",
      "2025-04-21 22:35:53.727066: Current learning rate: 0.00928\n",
      "2025-04-21 22:42:02.808550: train_loss -0.8313\n",
      "2025-04-21 22:42:02.811260: val_loss -0.7213\n",
      "2025-04-21 22:42:02.811320: Pseudo dice [np.float32(0.8061)]\n",
      "2025-04-21 22:42:02.811619: Epoch time: 369.08 s\n",
      "2025-04-21 22:42:02.811670: Yayy! New best EMA pseudo Dice: 0.7551000118255615\n",
      "2025-04-21 22:42:04.641435: \n",
      "2025-04-21 22:42:04.641665: Epoch 9\n",
      "2025-04-21 22:42:04.641748: Current learning rate: 0.00919\n",
      "2025-04-21 22:48:09.839060: train_loss -0.8383\n",
      "2025-04-21 22:48:09.842416: val_loss -0.7297\n",
      "2025-04-21 22:48:09.842677: Pseudo dice [np.float32(0.8153)]\n",
      "2025-04-21 22:48:09.843089: Epoch time: 365.2 s\n",
      "2025-04-21 22:48:09.843433: Yayy! New best EMA pseudo Dice: 0.7610999941825867\n",
      "2025-04-21 22:48:11.079350: \n",
      "2025-04-21 22:48:11.079597: Epoch 10\n",
      "2025-04-21 22:48:11.079682: Current learning rate: 0.0091\n",
      "2025-04-21 22:54:15.034527: train_loss -0.8467\n",
      "2025-04-21 22:54:15.038242: val_loss -0.7201\n",
      "2025-04-21 22:54:15.038485: Pseudo dice [np.float32(0.8086)]\n",
      "2025-04-21 22:54:15.038949: Epoch time: 363.96 s\n",
      "2025-04-21 22:54:15.039240: Yayy! New best EMA pseudo Dice: 0.7659000158309937\n",
      "2025-04-21 22:54:17.577042: \n",
      "2025-04-21 22:54:17.588437: Epoch 11\n",
      "2025-04-21 22:54:17.588572: Current learning rate: 0.009\n",
      "2025-04-21 23:00:41.261807: train_loss -0.8551\n",
      "2025-04-21 23:00:41.265664: val_loss -0.727\n",
      "2025-04-21 23:00:41.265936: Pseudo dice [np.float32(0.8119)]\n",
      "2025-04-21 23:00:41.266762: Epoch time: 383.68 s\n",
      "2025-04-21 23:00:41.267030: Yayy! New best EMA pseudo Dice: 0.7705000042915344\n",
      "2025-04-21 23:00:42.511854: \n",
      "2025-04-21 23:00:42.512095: Epoch 12\n",
      "2025-04-21 23:00:42.512252: Current learning rate: 0.00891\n",
      "2025-04-21 23:06:59.024538: train_loss -0.8572\n",
      "2025-04-21 23:06:59.027363: val_loss -0.7141\n",
      "2025-04-21 23:06:59.027692: Pseudo dice [np.float32(0.8055)]\n",
      "2025-04-21 23:06:59.028395: Epoch time: 376.51 s\n",
      "2025-04-21 23:06:59.028514: Yayy! New best EMA pseudo Dice: 0.7739999890327454\n",
      "2025-04-21 23:07:00.131402: \n",
      "2025-04-21 23:07:00.131644: Epoch 13\n",
      "2025-04-21 23:07:00.131726: Current learning rate: 0.00882\n",
      "2025-04-21 23:13:02.568856: train_loss -0.8625\n",
      "2025-04-21 23:13:02.572635: val_loss -0.7127\n",
      "2025-04-21 23:13:02.572734: Pseudo dice [np.float32(0.8064)]\n",
      "2025-04-21 23:13:02.573245: Epoch time: 362.44 s\n",
      "2025-04-21 23:13:02.573310: Yayy! New best EMA pseudo Dice: 0.7771999835968018\n",
      "2025-04-21 23:13:05.828719: \n",
      "2025-04-21 23:13:05.829064: Epoch 14\n",
      "2025-04-21 23:13:05.829195: Current learning rate: 0.00873\n",
      "2025-04-21 23:19:40.735147: train_loss -0.867\n",
      "2025-04-21 23:19:40.742385: val_loss -0.7132\n",
      "2025-04-21 23:19:40.742558: Pseudo dice [np.float32(0.8093)]\n",
      "2025-04-21 23:19:40.743290: Epoch time: 394.91 s\n",
      "2025-04-21 23:19:40.743370: Yayy! New best EMA pseudo Dice: 0.7803999781608582\n",
      "2025-04-21 23:19:42.074936: \n",
      "2025-04-21 23:19:42.075230: Epoch 15\n",
      "2025-04-21 23:19:42.075315: Current learning rate: 0.00864\n",
      "2025-04-21 23:26:07.823012: train_loss -0.8756\n",
      "2025-04-21 23:26:07.827093: val_loss -0.6957\n",
      "2025-04-21 23:26:07.827394: Pseudo dice [np.float32(0.798)]\n",
      "2025-04-21 23:26:07.827835: Epoch time: 385.75 s\n",
      "2025-04-21 23:26:07.827914: Yayy! New best EMA pseudo Dice: 0.7821999788284302\n",
      "2025-04-21 23:26:09.248411: \n",
      "2025-04-21 23:26:09.248663: Epoch 16\n",
      "2025-04-21 23:26:09.248752: Current learning rate: 0.00855\n",
      "2025-04-21 23:32:40.069669: train_loss -0.8755\n",
      "2025-04-21 23:32:40.072480: val_loss -0.7224\n",
      "2025-04-21 23:32:40.072858: Pseudo dice [np.float32(0.8124)]\n",
      "2025-04-21 23:32:40.073157: Epoch time: 390.82 s\n",
      "2025-04-21 23:32:40.073238: Yayy! New best EMA pseudo Dice: 0.7851999998092651\n",
      "2025-04-21 23:32:41.257434: \n",
      "2025-04-21 23:32:41.258039: Epoch 17\n",
      "2025-04-21 23:32:41.258285: Current learning rate: 0.00846\n",
      "2025-04-21 23:38:49.075470: train_loss -0.8779\n",
      "2025-04-21 23:38:49.078066: val_loss -0.7136\n",
      "2025-04-21 23:38:49.078318: Pseudo dice [np.float32(0.8088)]\n",
      "2025-04-21 23:38:49.078526: Epoch time: 367.82 s\n",
      "2025-04-21 23:38:49.078601: Yayy! New best EMA pseudo Dice: 0.7875999808311462\n",
      "2025-04-21 23:38:50.256673: \n",
      "2025-04-21 23:38:50.257022: Epoch 18\n",
      "2025-04-21 23:38:50.257103: Current learning rate: 0.00836\n",
      "2025-04-21 23:45:01.488271: train_loss -0.8783\n",
      "2025-04-21 23:45:01.490781: val_loss -0.7069\n",
      "2025-04-21 23:45:01.491039: Pseudo dice [np.float32(0.8071)]\n",
      "2025-04-21 23:45:01.491389: Epoch time: 371.23 s\n",
      "2025-04-21 23:45:01.491513: Yayy! New best EMA pseudo Dice: 0.7894999980926514\n",
      "2025-04-21 23:45:04.469940: \n",
      "2025-04-21 23:45:04.470147: Epoch 19\n",
      "2025-04-21 23:45:04.470249: Current learning rate: 0.00827\n",
      "2025-04-21 23:51:09.915269: train_loss -0.8831\n",
      "2025-04-21 23:51:09.919124: val_loss -0.7168\n",
      "2025-04-21 23:51:09.919408: Pseudo dice [np.float32(0.8133)]\n",
      "2025-04-21 23:51:09.919677: Epoch time: 365.46 s\n",
      "2025-04-21 23:51:09.919775: Yayy! New best EMA pseudo Dice: 0.7918999791145325\n",
      "2025-04-21 23:51:12.109399: \n",
      "2025-04-21 23:51:12.109556: Epoch 20\n",
      "2025-04-21 23:51:12.109632: Current learning rate: 0.00818\n",
      "2025-04-21 23:57:15.145117: train_loss -0.8871\n",
      "2025-04-21 23:57:15.147974: val_loss -0.7207\n",
      "2025-04-21 23:57:15.148085: Pseudo dice [np.float32(0.8162)]\n",
      "2025-04-21 23:57:15.148491: Epoch time: 363.04 s\n",
      "2025-04-21 23:57:15.148580: Yayy! New best EMA pseudo Dice: 0.7943000197410583\n",
      "2025-04-21 23:57:16.401696: \n",
      "2025-04-21 23:57:16.401938: Epoch 21\n",
      "2025-04-21 23:57:16.402042: Current learning rate: 0.00809\n",
      "2025-04-22 00:03:24.080248: train_loss -0.8885\n",
      "2025-04-22 00:03:24.085082: val_loss -0.7163\n",
      "2025-04-22 00:03:24.085725: Pseudo dice [np.float32(0.8144)]\n",
      "2025-04-22 00:03:24.086257: Epoch time: 367.68 s\n",
      "2025-04-22 00:03:24.086344: Yayy! New best EMA pseudo Dice: 0.7962999939918518\n",
      "2025-04-22 00:03:25.445315: \n",
      "2025-04-22 00:03:25.445595: Epoch 22\n",
      "2025-04-22 00:03:25.445681: Current learning rate: 0.008\n",
      "2025-04-22 00:09:35.024392: train_loss -0.8897\n",
      "2025-04-22 00:09:35.028252: val_loss -0.7105\n",
      "2025-04-22 00:09:35.028647: Pseudo dice [np.float32(0.8091)]\n",
      "2025-04-22 00:09:35.029142: Epoch time: 369.58 s\n",
      "2025-04-22 00:09:35.029465: Yayy! New best EMA pseudo Dice: 0.7975999712944031\n",
      "2025-04-22 00:09:36.384561: \n",
      "2025-04-22 00:09:36.384861: Epoch 23\n",
      "2025-04-22 00:09:36.384968: Current learning rate: 0.0079\n",
      "2025-04-22 00:15:47.422404: train_loss -0.889\n",
      "2025-04-22 00:15:47.425421: val_loss -0.7221\n",
      "2025-04-22 00:15:47.425721: Pseudo dice [np.float32(0.8183)]\n",
      "2025-04-22 00:15:47.426615: Epoch time: 371.04 s\n",
      "2025-04-22 00:15:47.426695: Yayy! New best EMA pseudo Dice: 0.7997000217437744\n",
      "2025-04-22 00:15:48.519430: \n",
      "2025-04-22 00:15:48.519672: Epoch 24\n",
      "2025-04-22 00:15:48.519745: Current learning rate: 0.00781\n",
      "2025-04-22 00:21:57.612115: train_loss -0.8923\n",
      "2025-04-22 00:21:57.615985: val_loss -0.7199\n",
      "2025-04-22 00:21:57.616069: Pseudo dice [np.float32(0.8186)]\n",
      "2025-04-22 00:21:57.616819: Epoch time: 369.09 s\n",
      "2025-04-22 00:21:57.617083: Yayy! New best EMA pseudo Dice: 0.8015999794006348\n",
      "2025-04-22 00:21:59.064406: \n",
      "2025-04-22 00:21:59.064653: Epoch 25\n",
      "2025-04-22 00:21:59.064739: Current learning rate: 0.00772\n",
      "2025-04-22 00:28:17.647997: train_loss -0.8899\n",
      "2025-04-22 00:28:17.652498: val_loss -0.7015\n",
      "2025-04-22 00:28:17.652622: Pseudo dice [np.float32(0.8103)]\n",
      "2025-04-22 00:28:17.652938: Epoch time: 378.58 s\n",
      "2025-04-22 00:28:17.653013: Yayy! New best EMA pseudo Dice: 0.8023999929428101\n",
      "2025-04-22 00:28:19.110741: \n",
      "2025-04-22 00:28:19.110883: Epoch 26\n",
      "2025-04-22 00:28:19.110966: Current learning rate: 0.00763\n",
      "2025-04-22 00:34:18.239852: train_loss -0.8963\n",
      "2025-04-22 00:34:18.244069: val_loss -0.707\n",
      "2025-04-22 00:34:18.244177: Pseudo dice [np.float32(0.8117)]\n",
      "2025-04-22 00:34:18.244623: Epoch time: 359.13 s\n",
      "2025-04-22 00:34:18.244705: Yayy! New best EMA pseudo Dice: 0.8033999800682068\n",
      "2025-04-22 00:34:20.641351: \n",
      "2025-04-22 00:34:20.641667: Epoch 27\n",
      "2025-04-22 00:34:20.641753: Current learning rate: 0.00753\n",
      "2025-04-22 00:40:21.494717: train_loss -0.8967\n",
      "2025-04-22 00:40:21.497026: val_loss -0.7086\n",
      "2025-04-22 00:40:21.497137: Pseudo dice [np.float32(0.8138)]\n",
      "2025-04-22 00:40:21.497403: Epoch time: 360.85 s\n",
      "2025-04-22 00:40:21.497552: Yayy! New best EMA pseudo Dice: 0.8044000267982483\n",
      "2025-04-22 00:40:22.713771: \n",
      "2025-04-22 00:40:22.714102: Epoch 28\n",
      "2025-04-22 00:40:22.714184: Current learning rate: 0.00744\n",
      "2025-04-22 00:46:17.725937: train_loss -0.8991\n",
      "2025-04-22 00:46:17.728966: val_loss -0.713\n",
      "2025-04-22 00:46:17.729139: Pseudo dice [np.float32(0.8151)]\n",
      "2025-04-22 00:46:17.729603: Epoch time: 355.01 s\n",
      "2025-04-22 00:46:17.729882: Yayy! New best EMA pseudo Dice: 0.8054999709129333\n",
      "2025-04-22 00:46:18.948373: \n",
      "2025-04-22 00:46:18.948854: Epoch 29\n",
      "2025-04-22 00:46:18.948953: Current learning rate: 0.00735\n",
      "2025-04-22 00:52:15.894085: train_loss -0.8959\n",
      "2025-04-22 00:52:15.896687: val_loss -0.7244\n",
      "2025-04-22 00:52:15.896789: Pseudo dice [np.float32(0.8216)]\n",
      "2025-04-22 00:52:15.897158: Epoch time: 356.95 s\n",
      "2025-04-22 00:52:15.897430: Yayy! New best EMA pseudo Dice: 0.8070999979972839\n",
      "2025-04-22 00:52:17.237605: \n",
      "2025-04-22 00:52:17.237874: Epoch 30\n",
      "2025-04-22 00:52:17.237961: Current learning rate: 0.00725\n",
      "2025-04-22 00:58:13.009145: train_loss -0.8989\n",
      "2025-04-22 00:58:13.011748: val_loss -0.7111\n",
      "2025-04-22 00:58:13.011892: Pseudo dice [np.float32(0.8144)]\n",
      "2025-04-22 00:58:13.012119: Epoch time: 355.77 s\n",
      "2025-04-22 00:58:13.012208: Yayy! New best EMA pseudo Dice: 0.8077999949455261\n",
      "2025-04-22 00:58:14.281657: \n",
      "2025-04-22 00:58:14.281905: Epoch 31\n",
      "2025-04-22 00:58:14.281986: Current learning rate: 0.00716\n",
      "2025-04-22 01:04:10.958202: train_loss -0.901\n",
      "2025-04-22 01:04:10.960749: val_loss -0.7088\n",
      "2025-04-22 01:04:10.960867: Pseudo dice [np.float32(0.8164)]\n",
      "2025-04-22 01:04:10.961091: Epoch time: 356.68 s\n",
      "2025-04-22 01:04:10.961183: Yayy! New best EMA pseudo Dice: 0.8087000250816345\n",
      "2025-04-22 01:04:12.051337: \n",
      "2025-04-22 01:04:12.051646: Epoch 32\n",
      "2025-04-22 01:04:12.051728: Current learning rate: 0.00707\n",
      "2025-04-22 01:10:08.082785: train_loss -0.9035\n",
      "2025-04-22 01:10:08.085195: val_loss -0.7051\n",
      "2025-04-22 01:10:08.085304: Pseudo dice [np.float32(0.8111)]\n",
      "2025-04-22 01:10:08.085576: Epoch time: 356.03 s\n",
      "2025-04-22 01:10:08.085668: Yayy! New best EMA pseudo Dice: 0.808899998664856\n",
      "2025-04-22 01:10:09.775148: \n",
      "2025-04-22 01:10:09.784225: Epoch 33\n",
      "2025-04-22 01:10:09.784321: Current learning rate: 0.00697\n",
      "2025-04-22 01:16:06.926847: train_loss -0.9012\n",
      "2025-04-22 01:16:06.929704: val_loss -0.6944\n",
      "2025-04-22 01:16:06.929786: Pseudo dice [np.float32(0.8058)]\n",
      "2025-04-22 01:16:06.930212: Epoch time: 357.15 s\n",
      "2025-04-22 01:16:07.433488: \n",
      "2025-04-22 01:16:07.433631: Epoch 34\n",
      "2025-04-22 01:16:07.433714: Current learning rate: 0.00688\n",
      "2025-04-22 01:22:04.027740: train_loss -0.9053\n",
      "2025-04-22 01:22:04.029768: val_loss -0.7027\n",
      "2025-04-22 01:22:04.029904: Pseudo dice [np.float32(0.8124)]\n",
      "2025-04-22 01:22:04.030125: Epoch time: 356.6 s\n",
      "2025-04-22 01:22:04.030206: Yayy! New best EMA pseudo Dice: 0.8090000152587891\n",
      "2025-04-22 01:22:05.035814: \n",
      "2025-04-22 01:22:05.036083: Epoch 35\n",
      "2025-04-22 01:22:05.036154: Current learning rate: 0.00679\n",
      "2025-04-22 01:27:58.874930: train_loss -0.9046\n",
      "2025-04-22 01:27:58.877434: val_loss -0.7171\n",
      "2025-04-22 01:27:58.877537: Pseudo dice [np.float32(0.8205)]\n",
      "2025-04-22 01:27:58.877764: Epoch time: 353.84 s\n",
      "2025-04-22 01:27:58.877839: Yayy! New best EMA pseudo Dice: 0.8101000189781189\n",
      "2025-04-22 01:28:00.003959: \n",
      "2025-04-22 01:28:00.006062: Epoch 36\n",
      "2025-04-22 01:28:00.006189: Current learning rate: 0.00669\n",
      "2025-04-22 01:33:55.367129: train_loss -0.907\n",
      "2025-04-22 01:33:55.370346: val_loss -0.708\n",
      "2025-04-22 01:33:55.370449: Pseudo dice [np.float32(0.8144)]\n",
      "2025-04-22 01:33:55.370713: Epoch time: 355.36 s\n",
      "2025-04-22 01:33:55.370782: Yayy! New best EMA pseudo Dice: 0.8105999827384949\n",
      "2025-04-22 01:33:56.558823: \n",
      "2025-04-22 01:33:56.559079: Epoch 37\n",
      "2025-04-22 01:33:56.559161: Current learning rate: 0.0066\n",
      "2025-04-22 01:39:52.579155: train_loss -0.9068\n",
      "2025-04-22 01:39:52.581414: val_loss -0.7081\n",
      "2025-04-22 01:39:52.581545: Pseudo dice [np.float32(0.8164)]\n",
      "2025-04-22 01:39:52.581799: Epoch time: 356.02 s\n",
      "2025-04-22 01:39:52.581887: Yayy! New best EMA pseudo Dice: 0.8111000061035156\n",
      "2025-04-22 01:39:53.712524: \n",
      "2025-04-22 01:39:53.712781: Epoch 38\n",
      "2025-04-22 01:39:53.712872: Current learning rate: 0.0065\n",
      "2025-04-22 01:45:49.109872: train_loss -0.9064\n",
      "2025-04-22 01:45:49.112469: val_loss -0.7018\n",
      "2025-04-22 01:45:49.112574: Pseudo dice [np.float32(0.811)]\n",
      "2025-04-22 01:45:49.112830: Epoch time: 355.4 s\n",
      "2025-04-22 01:45:49.532727: \n",
      "2025-04-22 01:45:49.532847: Epoch 39\n",
      "2025-04-22 01:45:49.532922: Current learning rate: 0.00641\n",
      "2025-04-22 01:51:43.351400: train_loss -0.9085\n",
      "2025-04-22 01:51:43.353912: val_loss -0.7056\n",
      "2025-04-22 01:51:43.354006: Pseudo dice [np.float32(0.8131)]\n",
      "2025-04-22 01:51:43.354234: Epoch time: 353.82 s\n",
      "2025-04-22 01:51:43.354394: Yayy! New best EMA pseudo Dice: 0.8112999796867371\n",
      "2025-04-22 01:51:45.345778: \n",
      "2025-04-22 01:51:45.346039: Epoch 40\n",
      "2025-04-22 01:51:45.346114: Current learning rate: 0.00631\n",
      "2025-04-22 01:57:41.825022: train_loss -0.9092\n",
      "2025-04-22 01:57:41.827820: val_loss -0.7128\n",
      "2025-04-22 01:57:41.827937: Pseudo dice [np.float32(0.8201)]\n",
      "2025-04-22 01:57:41.828294: Epoch time: 356.48 s\n",
      "2025-04-22 01:57:41.828574: Yayy! New best EMA pseudo Dice: 0.8122000098228455\n",
      "2025-04-22 01:57:43.076702: \n",
      "2025-04-22 01:57:43.076918: Epoch 41\n",
      "2025-04-22 01:57:43.076992: Current learning rate: 0.00622\n",
      "2025-04-22 02:03:41.312669: train_loss -0.9076\n",
      "2025-04-22 02:03:41.315443: val_loss -0.7107\n",
      "2025-04-22 02:03:41.315704: Pseudo dice [np.float32(0.8151)]\n",
      "2025-04-22 02:03:41.316192: Epoch time: 358.24 s\n",
      "2025-04-22 02:03:41.316277: Yayy! New best EMA pseudo Dice: 0.8125\n",
      "2025-04-22 02:03:42.533496: \n",
      "2025-04-22 02:03:42.533718: Epoch 42\n",
      "2025-04-22 02:03:42.533792: Current learning rate: 0.00612\n",
      "2025-04-22 02:09:40.159391: train_loss -0.9076\n",
      "2025-04-22 02:09:40.161694: val_loss -0.7147\n",
      "2025-04-22 02:09:40.161812: Pseudo dice [np.float32(0.8188)]\n",
      "2025-04-22 02:09:40.162021: Epoch time: 357.63 s\n",
      "2025-04-22 02:09:40.162091: Yayy! New best EMA pseudo Dice: 0.8130999803543091\n",
      "2025-04-22 02:09:41.357178: \n",
      "2025-04-22 02:09:41.357400: Epoch 43\n",
      "2025-04-22 02:09:41.357477: Current learning rate: 0.00603\n",
      "2025-04-22 02:15:39.055201: train_loss -0.9134\n",
      "2025-04-22 02:15:39.058295: val_loss -0.7139\n",
      "2025-04-22 02:15:39.058404: Pseudo dice [np.float32(0.8196)]\n",
      "2025-04-22 02:15:39.059064: Epoch time: 357.7 s\n",
      "2025-04-22 02:15:39.059178: Yayy! New best EMA pseudo Dice: 0.8137999773025513\n",
      "2025-04-22 02:15:40.213768: \n",
      "2025-04-22 02:15:40.214024: Epoch 44\n",
      "2025-04-22 02:15:40.214128: Current learning rate: 0.00593\n",
      "2025-04-22 02:21:39.224927: train_loss -0.9121\n",
      "2025-04-22 02:21:39.227211: val_loss -0.7065\n",
      "2025-04-22 02:21:39.227273: Pseudo dice [np.float32(0.8146)]\n",
      "2025-04-22 02:21:39.227579: Epoch time: 359.01 s\n",
      "2025-04-22 02:21:39.227630: Yayy! New best EMA pseudo Dice: 0.8138999938964844\n",
      "2025-04-22 02:21:40.276033: \n",
      "2025-04-22 02:21:40.276351: Epoch 45\n",
      "2025-04-22 02:21:40.276469: Current learning rate: 0.00584\n",
      "2025-04-22 02:27:38.204292: train_loss -0.9126\n",
      "2025-04-22 02:27:38.206506: val_loss -0.6998\n",
      "2025-04-22 02:27:38.206631: Pseudo dice [np.float32(0.8128)]\n",
      "2025-04-22 02:27:38.206883: Epoch time: 357.93 s\n",
      "2025-04-22 02:27:39.317978: \n",
      "2025-04-22 02:27:39.318133: Epoch 46\n",
      "2025-04-22 02:27:39.318215: Current learning rate: 0.00574\n",
      "2025-04-22 02:33:37.882813: train_loss -0.9117\n",
      "2025-04-22 02:33:37.886826: val_loss -0.7032\n",
      "2025-04-22 02:33:37.886930: Pseudo dice [np.float32(0.8125)]\n",
      "2025-04-22 02:33:37.887165: Epoch time: 358.57 s\n",
      "2025-04-22 02:33:38.404613: \n",
      "2025-04-22 02:33:38.404733: Epoch 47\n",
      "2025-04-22 02:33:38.404807: Current learning rate: 0.00565\n",
      "2025-04-22 02:39:35.378261: train_loss -0.9139\n",
      "2025-04-22 02:39:35.381621: val_loss -0.7037\n",
      "2025-04-22 02:39:35.381860: Pseudo dice [np.float32(0.8162)]\n",
      "2025-04-22 02:39:35.382543: Epoch time: 356.97 s\n",
      "2025-04-22 02:39:35.382651: Yayy! New best EMA pseudo Dice: 0.8138999938964844\n",
      "2025-04-22 02:39:36.554451: \n",
      "2025-04-22 02:39:36.557248: Epoch 48\n",
      "2025-04-22 02:39:36.557378: Current learning rate: 0.00555\n",
      "2025-04-22 02:45:35.216820: train_loss -0.9132\n",
      "2025-04-22 02:45:35.219397: val_loss -0.7142\n",
      "2025-04-22 02:45:35.219507: Pseudo dice [np.float32(0.818)]\n",
      "2025-04-22 02:45:35.219750: Epoch time: 358.66 s\n",
      "2025-04-22 02:45:35.219828: Yayy! New best EMA pseudo Dice: 0.814300000667572\n",
      "2025-04-22 02:45:36.394046: \n",
      "2025-04-22 02:45:36.394293: Epoch 49\n",
      "2025-04-22 02:45:36.394394: Current learning rate: 0.00546\n",
      "2025-04-22 02:51:34.465002: train_loss -0.9147\n",
      "2025-04-22 02:51:34.467532: val_loss -0.7057\n",
      "2025-04-22 02:51:34.467627: Pseudo dice [np.float32(0.8143)]\n",
      "2025-04-22 02:51:34.467988: Epoch time: 358.07 s\n",
      "2025-04-22 02:51:35.559359: Yayy! New best EMA pseudo Dice: 0.814300000667572\n",
      "2025-04-22 02:51:37.202577: \n",
      "2025-04-22 02:51:37.202941: Epoch 50\n",
      "2025-04-22 02:51:37.203092: Current learning rate: 0.00536\n",
      "2025-04-22 02:57:39.240762: train_loss -0.9163\n",
      "2025-04-22 02:57:39.243817: val_loss -0.6994\n",
      "2025-04-22 02:57:39.244113: Pseudo dice [np.float32(0.8102)]\n",
      "2025-04-22 02:57:39.244537: Epoch time: 362.04 s\n",
      "2025-04-22 02:57:39.721703: \n",
      "2025-04-22 02:57:39.721836: Epoch 51\n",
      "2025-04-22 02:57:39.721913: Current learning rate: 0.00526\n",
      "2025-04-22 03:03:38.467094: train_loss -0.9163\n",
      "2025-04-22 03:03:38.470901: val_loss -0.717\n",
      "2025-04-22 03:03:38.470985: Pseudo dice [np.float32(0.8208)]\n",
      "2025-04-22 03:03:38.471440: Epoch time: 358.75 s\n",
      "2025-04-22 03:03:38.472140: Yayy! New best EMA pseudo Dice: 0.8145999908447266\n",
      "2025-04-22 03:03:40.637820: \n",
      "2025-04-22 03:03:40.638111: Epoch 52\n",
      "2025-04-22 03:03:40.638209: Current learning rate: 0.00517\n",
      "2025-04-22 03:09:39.012134: train_loss -0.9167\n",
      "2025-04-22 03:09:39.015078: val_loss -0.7092\n",
      "2025-04-22 03:09:39.015183: Pseudo dice [np.float32(0.8164)]\n",
      "2025-04-22 03:09:39.015399: Epoch time: 358.37 s\n",
      "2025-04-22 03:09:39.015509: Yayy! New best EMA pseudo Dice: 0.8148000240325928\n",
      "2025-04-22 03:09:39.745627: \n",
      "2025-04-22 03:09:39.745857: Epoch 53\n",
      "2025-04-22 03:09:39.745937: Current learning rate: 0.00507\n",
      "2025-04-22 03:15:37.534320: train_loss -0.9187\n",
      "2025-04-22 03:15:37.536258: val_loss -0.7021\n",
      "2025-04-22 03:15:37.536366: Pseudo dice [np.float32(0.8134)]\n",
      "2025-04-22 03:15:37.536607: Epoch time: 357.79 s\n",
      "2025-04-22 03:15:37.955450: \n",
      "2025-04-22 03:15:37.955583: Epoch 54\n",
      "2025-04-22 03:15:37.955662: Current learning rate: 0.00497\n",
      "2025-04-22 03:21:38.605684: train_loss -0.9178\n",
      "2025-04-22 03:21:38.608364: val_loss -0.7141\n",
      "2025-04-22 03:21:38.608657: Pseudo dice [np.float32(0.819)]\n",
      "2025-04-22 03:21:38.608863: Epoch time: 360.65 s\n",
      "2025-04-22 03:21:38.608940: Yayy! New best EMA pseudo Dice: 0.8151000142097473\n",
      "2025-04-22 03:21:39.509225: \n",
      "2025-04-22 03:21:39.509523: Epoch 55\n",
      "2025-04-22 03:21:39.509600: Current learning rate: 0.00487\n",
      "2025-04-22 03:27:35.762786: train_loss -0.9198\n",
      "2025-04-22 03:27:35.765329: val_loss -0.7089\n",
      "2025-04-22 03:27:35.765450: Pseudo dice [np.float32(0.8191)]\n",
      "2025-04-22 03:27:35.765863: Epoch time: 356.25 s\n",
      "2025-04-22 03:27:35.765943: Yayy! New best EMA pseudo Dice: 0.815500020980835\n",
      "2025-04-22 03:27:36.602629: \n",
      "2025-04-22 03:27:36.602895: Epoch 56\n",
      "2025-04-22 03:27:36.602979: Current learning rate: 0.00478\n",
      "2025-04-22 03:33:31.572075: train_loss -0.9185\n",
      "2025-04-22 03:33:31.574446: val_loss -0.709\n",
      "2025-04-22 03:33:31.574560: Pseudo dice [np.float32(0.815)]\n",
      "2025-04-22 03:33:31.574795: Epoch time: 354.97 s\n",
      "2025-04-22 03:33:31.990286: \n",
      "2025-04-22 03:33:31.990420: Epoch 57\n",
      "2025-04-22 03:33:31.990494: Current learning rate: 0.00468\n",
      "2025-04-22 03:39:25.660340: train_loss -0.9213\n",
      "2025-04-22 03:39:25.663232: val_loss -0.7161\n",
      "2025-04-22 03:39:25.663357: Pseudo dice [np.float32(0.8213)]\n",
      "2025-04-22 03:39:25.663643: Epoch time: 353.67 s\n",
      "2025-04-22 03:39:25.663732: Yayy! New best EMA pseudo Dice: 0.8159999847412109\n",
      "2025-04-22 03:39:26.430482: \n",
      "2025-04-22 03:39:26.430811: Epoch 58\n",
      "2025-04-22 03:39:26.430943: Current learning rate: 0.00458\n",
      "2025-04-22 03:45:20.245928: train_loss -0.9198\n",
      "2025-04-22 03:45:20.249273: val_loss -0.7066\n",
      "2025-04-22 03:45:20.249413: Pseudo dice [np.float32(0.8128)]\n",
      "2025-04-22 03:45:20.249690: Epoch time: 353.82 s\n",
      "2025-04-22 03:45:21.400594: \n",
      "2025-04-22 03:45:21.400723: Epoch 59\n",
      "2025-04-22 03:45:21.400808: Current learning rate: 0.00448\n",
      "2025-04-22 03:51:18.285398: train_loss -0.9235\n",
      "2025-04-22 03:51:18.290124: val_loss -0.7072\n",
      "2025-04-22 03:51:18.290257: Pseudo dice [np.float32(0.8164)]\n",
      "2025-04-22 03:51:18.290543: Epoch time: 356.88 s\n",
      "2025-04-22 03:51:18.815150: \n",
      "2025-04-22 03:51:18.815301: Epoch 60\n",
      "2025-04-22 03:51:18.815379: Current learning rate: 0.00438\n",
      "2025-04-22 03:57:08.869393: train_loss -0.9212\n",
      "2025-04-22 03:57:08.872201: val_loss -0.7171\n",
      "2025-04-22 03:57:08.872316: Pseudo dice [np.float32(0.8219)]\n",
      "2025-04-22 03:57:08.872623: Epoch time: 350.06 s\n",
      "2025-04-22 03:57:08.872789: Yayy! New best EMA pseudo Dice: 0.8163999915122986\n",
      "2025-04-22 03:57:09.633194: \n",
      "2025-04-22 03:57:09.634181: Epoch 61\n",
      "2025-04-22 03:57:09.634370: Current learning rate: 0.00429\n",
      "2025-04-22 04:02:58.539600: train_loss -0.9242\n",
      "2025-04-22 04:02:58.542420: val_loss -0.7022\n",
      "2025-04-22 04:02:58.542558: Pseudo dice [np.float32(0.8142)]\n",
      "2025-04-22 04:02:58.542789: Epoch time: 348.91 s\n",
      "2025-04-22 04:02:58.957360: \n",
      "2025-04-22 04:02:58.957517: Epoch 62\n",
      "2025-04-22 04:02:58.957597: Current learning rate: 0.00419\n",
      "2025-04-22 04:08:47.541207: train_loss -0.9225\n",
      "2025-04-22 04:08:47.543230: val_loss -0.7144\n",
      "2025-04-22 04:08:47.543360: Pseudo dice [np.float32(0.8181)]\n",
      "2025-04-22 04:08:47.543498: Epoch time: 348.58 s\n",
      "2025-04-22 04:08:47.943937: \n",
      "2025-04-22 04:08:47.944096: Epoch 63\n",
      "2025-04-22 04:08:47.944207: Current learning rate: 0.00409\n",
      "2025-04-22 04:14:36.983973: train_loss -0.9222\n",
      "2025-04-22 04:14:36.987058: val_loss -0.7122\n",
      "2025-04-22 04:14:36.987184: Pseudo dice [np.float32(0.8171)]\n",
      "2025-04-22 04:14:36.987483: Epoch time: 349.04 s\n",
      "2025-04-22 04:14:36.987701: Yayy! New best EMA pseudo Dice: 0.8163999915122986\n",
      "2025-04-22 04:14:37.738436: \n",
      "2025-04-22 04:14:37.739664: Epoch 64\n",
      "2025-04-22 04:14:37.739762: Current learning rate: 0.00399\n",
      "2025-04-22 04:20:27.055393: train_loss -0.9227\n",
      "2025-04-22 04:20:27.058085: val_loss -0.7068\n",
      "2025-04-22 04:20:27.058205: Pseudo dice [np.float32(0.8167)]\n",
      "2025-04-22 04:20:27.058530: Epoch time: 349.32 s\n",
      "2025-04-22 04:20:27.058621: Yayy! New best EMA pseudo Dice: 0.8163999915122986\n",
      "2025-04-22 04:20:27.809761: \n",
      "2025-04-22 04:20:27.810126: Epoch 65\n",
      "2025-04-22 04:20:27.810217: Current learning rate: 0.00389\n",
      "2025-04-22 04:26:18.324675: train_loss -0.9246\n",
      "2025-04-22 04:26:18.327399: val_loss -0.712\n",
      "2025-04-22 04:26:18.327543: Pseudo dice [np.float32(0.8176)]\n",
      "2025-04-22 04:26:18.327838: Epoch time: 350.52 s\n",
      "2025-04-22 04:26:18.328006: Yayy! New best EMA pseudo Dice: 0.8166000247001648\n",
      "2025-04-22 04:26:19.689060: \n",
      "2025-04-22 04:26:19.689353: Epoch 66\n",
      "2025-04-22 04:26:19.689464: Current learning rate: 0.00379\n",
      "2025-04-22 04:32:10.769072: train_loss -0.9226\n",
      "2025-04-22 04:32:10.771499: val_loss -0.7124\n",
      "2025-04-22 04:32:10.771625: Pseudo dice [np.float32(0.8165)]\n",
      "2025-04-22 04:32:10.771925: Epoch time: 351.08 s\n",
      "2025-04-22 04:32:11.178586: \n",
      "2025-04-22 04:32:11.178710: Epoch 67\n",
      "2025-04-22 04:32:11.178790: Current learning rate: 0.00369\n",
      "2025-04-22 04:38:02.056294: train_loss -0.9225\n",
      "2025-04-22 04:38:02.059006: val_loss -0.7098\n",
      "2025-04-22 04:38:02.059147: Pseudo dice [np.float32(0.8161)]\n",
      "2025-04-22 04:38:02.059472: Epoch time: 350.88 s\n",
      "2025-04-22 04:38:02.459221: \n",
      "2025-04-22 04:38:02.459377: Epoch 68\n",
      "2025-04-22 04:38:02.459459: Current learning rate: 0.00359\n",
      "2025-04-22 04:43:53.131080: train_loss -0.9262\n",
      "2025-04-22 04:43:53.134674: val_loss -0.7207\n",
      "2025-04-22 04:43:53.134785: Pseudo dice [np.float32(0.8263)]\n",
      "2025-04-22 04:43:53.135056: Epoch time: 350.67 s\n",
      "2025-04-22 04:43:53.135136: Yayy! New best EMA pseudo Dice: 0.8174999952316284\n",
      "2025-04-22 04:43:53.842831: \n",
      "2025-04-22 04:43:53.843093: Epoch 69\n",
      "2025-04-22 04:43:53.843165: Current learning rate: 0.00349\n",
      "2025-04-22 04:49:44.896801: train_loss -0.9262\n",
      "2025-04-22 04:49:44.898777: val_loss -0.7216\n",
      "2025-04-22 04:49:44.898896: Pseudo dice [np.float32(0.8209)]\n",
      "2025-04-22 04:49:44.899172: Epoch time: 351.05 s\n",
      "2025-04-22 04:49:44.899269: Yayy! New best EMA pseudo Dice: 0.817799985408783\n",
      "2025-04-22 04:49:45.623236: \n",
      "2025-04-22 04:49:45.623543: Epoch 70\n",
      "2025-04-22 04:49:45.623654: Current learning rate: 0.00338\n",
      "2025-04-22 04:55:39.416010: train_loss -0.9274\n",
      "2025-04-22 04:55:39.418479: val_loss -0.7242\n",
      "2025-04-22 04:55:39.418620: Pseudo dice [np.float32(0.8251)]\n",
      "2025-04-22 04:55:39.418901: Epoch time: 353.79 s\n",
      "2025-04-22 04:55:39.419009: Yayy! New best EMA pseudo Dice: 0.8185999989509583\n",
      "2025-04-22 04:55:40.176048: \n",
      "2025-04-22 04:55:40.176290: Epoch 71\n",
      "2025-04-22 04:55:40.176378: Current learning rate: 0.00328\n",
      "2025-04-22 05:01:33.269596: train_loss -0.9257\n",
      "2025-04-22 05:01:33.271677: val_loss -0.7\n",
      "2025-04-22 05:01:33.271819: Pseudo dice [np.float32(0.8101)]\n",
      "2025-04-22 05:01:33.272155: Epoch time: 353.09 s\n",
      "2025-04-22 05:01:34.205980: \n",
      "2025-04-22 05:01:34.206140: Epoch 72\n",
      "2025-04-22 05:01:34.206230: Current learning rate: 0.00318\n",
      "2025-04-22 05:07:25.912444: train_loss -0.9269\n",
      "2025-04-22 05:07:25.914598: val_loss -0.7114\n",
      "2025-04-22 05:07:25.914696: Pseudo dice [np.float32(0.8203)]\n",
      "2025-04-22 05:07:25.914919: Epoch time: 351.71 s\n",
      "2025-04-22 05:07:26.337514: \n",
      "2025-04-22 05:07:26.337657: Epoch 73\n",
      "2025-04-22 05:07:26.337754: Current learning rate: 0.00308\n",
      "2025-04-22 05:13:17.756864: train_loss -0.9277\n",
      "2025-04-22 05:13:17.759182: val_loss -0.7051\n",
      "2025-04-22 05:13:17.759329: Pseudo dice [np.float32(0.814)]\n",
      "2025-04-22 05:13:17.759615: Epoch time: 351.42 s\n",
      "2025-04-22 05:13:18.163799: \n",
      "2025-04-22 05:13:18.163939: Epoch 74\n",
      "2025-04-22 05:13:18.164031: Current learning rate: 0.00297\n",
      "2025-04-22 05:19:09.708987: train_loss -0.9279\n",
      "2025-04-22 05:19:09.711525: val_loss -0.719\n",
      "2025-04-22 05:19:09.711647: Pseudo dice [np.float32(0.8201)]\n",
      "2025-04-22 05:19:09.712030: Epoch time: 351.55 s\n",
      "2025-04-22 05:19:10.161578: \n",
      "2025-04-22 05:19:10.161692: Epoch 75\n",
      "2025-04-22 05:19:10.161759: Current learning rate: 0.00287\n",
      "2025-04-22 05:25:01.497728: train_loss -0.9303\n",
      "2025-04-22 05:25:01.500212: val_loss -0.7166\n",
      "2025-04-22 05:25:01.500324: Pseudo dice [np.float32(0.819)]\n",
      "2025-04-22 05:25:01.500753: Epoch time: 351.34 s\n",
      "2025-04-22 05:25:01.917013: \n",
      "2025-04-22 05:25:01.917144: Epoch 76\n",
      "2025-04-22 05:25:01.917214: Current learning rate: 0.00277\n",
      "2025-04-22 05:30:51.389365: train_loss -0.9309\n",
      "2025-04-22 05:30:51.391698: val_loss -0.7152\n",
      "2025-04-22 05:30:51.391808: Pseudo dice [np.float32(0.819)]\n",
      "2025-04-22 05:30:51.392052: Epoch time: 349.47 s\n",
      "2025-04-22 05:30:51.848417: \n",
      "2025-04-22 05:30:51.848532: Epoch 77\n",
      "2025-04-22 05:30:51.848605: Current learning rate: 0.00266\n",
      "2025-04-22 05:36:40.806228: train_loss -0.9303\n",
      "2025-04-22 05:36:40.808943: val_loss -0.7131\n",
      "2025-04-22 05:36:40.809077: Pseudo dice [np.float32(0.8204)]\n",
      "2025-04-22 05:36:40.809349: Epoch time: 348.96 s\n",
      "2025-04-22 05:36:41.233392: \n",
      "2025-04-22 05:36:41.233522: Epoch 78\n",
      "2025-04-22 05:36:41.233603: Current learning rate: 0.00256\n",
      "2025-04-22 05:42:30.013460: train_loss -0.9304\n",
      "2025-04-22 05:42:30.015386: val_loss -0.6998\n",
      "2025-04-22 05:42:30.015548: Pseudo dice [np.float32(0.8125)]\n",
      "2025-04-22 05:42:30.015704: Epoch time: 348.78 s\n",
      "2025-04-22 05:42:30.933342: \n",
      "2025-04-22 05:42:30.933465: Epoch 79\n",
      "2025-04-22 05:42:30.933538: Current learning rate: 0.00245\n",
      "2025-04-22 05:48:19.814064: train_loss -0.9308\n",
      "2025-04-22 05:48:19.815595: val_loss -0.7144\n",
      "2025-04-22 05:48:19.815713: Pseudo dice [np.float32(0.8221)]\n",
      "2025-04-22 05:48:19.815866: Epoch time: 348.88 s\n",
      "2025-04-22 05:48:20.242348: \n",
      "2025-04-22 05:48:20.242482: Epoch 80\n",
      "2025-04-22 05:48:20.242579: Current learning rate: 0.00235\n",
      "2025-04-22 05:54:09.032278: train_loss -0.9324\n",
      "2025-04-22 05:54:09.033589: val_loss -0.7147\n",
      "2025-04-22 05:54:09.033705: Pseudo dice [np.float32(0.8205)]\n",
      "2025-04-22 05:54:09.033843: Epoch time: 348.79 s\n",
      "2025-04-22 05:54:09.464174: \n",
      "2025-04-22 05:54:09.464314: Epoch 81\n",
      "2025-04-22 05:54:09.464398: Current learning rate: 0.00224\n",
      "2025-04-22 05:59:58.299249: train_loss -0.9308\n",
      "2025-04-22 05:59:58.302548: val_loss -0.7093\n",
      "2025-04-22 05:59:58.302677: Pseudo dice [np.float32(0.8173)]\n",
      "2025-04-22 05:59:58.302924: Epoch time: 348.84 s\n",
      "2025-04-22 05:59:58.716060: \n",
      "2025-04-22 05:59:58.716180: Epoch 82\n",
      "2025-04-22 05:59:58.716252: Current learning rate: 0.00214\n",
      "2025-04-22 06:05:47.503531: train_loss -0.9326\n",
      "2025-04-22 06:05:47.504799: val_loss -0.7093\n",
      "2025-04-22 06:05:47.504947: Pseudo dice [np.float32(0.8159)]\n",
      "2025-04-22 06:05:47.505106: Epoch time: 348.79 s\n",
      "2025-04-22 06:05:47.902445: \n",
      "2025-04-22 06:05:47.902626: Epoch 83\n",
      "2025-04-22 06:05:47.902696: Current learning rate: 0.00203\n",
      "2025-04-22 06:11:38.384689: train_loss -0.9323\n",
      "2025-04-22 06:11:38.387177: val_loss -0.7221\n",
      "2025-04-22 06:11:38.387291: Pseudo dice [np.float32(0.8237)]\n",
      "2025-04-22 06:11:38.387566: Epoch time: 350.48 s\n",
      "2025-04-22 06:11:38.387647: Yayy! New best EMA pseudo Dice: 0.8185999989509583\n",
      "2025-04-22 06:11:39.134738: \n",
      "2025-04-22 06:11:39.134972: Epoch 84\n",
      "2025-04-22 06:11:39.135095: Current learning rate: 0.00192\n",
      "2025-04-22 06:17:31.105851: train_loss -0.9338\n",
      "2025-04-22 06:17:31.108796: val_loss -0.7201\n",
      "2025-04-22 06:17:31.108891: Pseudo dice [np.float32(0.8217)]\n",
      "2025-04-22 06:17:31.109216: Epoch time: 351.97 s\n",
      "2025-04-22 06:17:31.109290: Yayy! New best EMA pseudo Dice: 0.8188999891281128\n",
      "2025-04-22 06:17:32.700824: \n",
      "2025-04-22 06:17:32.701110: Epoch 85\n",
      "2025-04-22 06:17:32.701190: Current learning rate: 0.00181\n",
      "2025-04-22 06:23:23.693744: train_loss -0.9342\n",
      "2025-04-22 06:23:23.696661: val_loss -0.7219\n",
      "2025-04-22 06:23:23.696794: Pseudo dice [np.float32(0.8224)]\n",
      "2025-04-22 06:23:23.697033: Epoch time: 350.99 s\n",
      "2025-04-22 06:23:23.697117: Yayy! New best EMA pseudo Dice: 0.8192999958992004\n",
      "2025-04-22 06:23:24.521300: \n",
      "2025-04-22 06:23:24.521657: Epoch 86\n",
      "2025-04-22 06:23:24.521792: Current learning rate: 0.0017\n",
      "2025-04-22 06:29:13.399361: train_loss -0.9341\n",
      "2025-04-22 06:29:13.401774: val_loss -0.7152\n",
      "2025-04-22 06:29:13.401896: Pseudo dice [np.float32(0.8234)]\n",
      "2025-04-22 06:29:13.402186: Epoch time: 348.88 s\n",
      "2025-04-22 06:29:13.402353: Yayy! New best EMA pseudo Dice: 0.8197000026702881\n",
      "2025-04-22 06:29:14.184198: \n",
      "2025-04-22 06:29:14.184479: Epoch 87\n",
      "2025-04-22 06:29:14.184580: Current learning rate: 0.00159\n",
      "2025-04-22 06:35:05.056298: train_loss -0.9356\n",
      "2025-04-22 06:35:05.058438: val_loss -0.7198\n",
      "2025-04-22 06:35:05.058555: Pseudo dice [np.float32(0.8234)]\n",
      "2025-04-22 06:35:05.058855: Epoch time: 350.87 s\n",
      "2025-04-22 06:35:05.059021: Yayy! New best EMA pseudo Dice: 0.8199999928474426\n",
      "2025-04-22 06:35:05.917934: \n",
      "2025-04-22 06:35:05.918250: Epoch 88\n",
      "2025-04-22 06:35:05.918355: Current learning rate: 0.00148\n",
      "2025-04-22 06:40:57.339258: train_loss -0.9336\n",
      "2025-04-22 06:40:57.340261: val_loss -0.7174\n",
      "2025-04-22 06:40:57.340380: Pseudo dice [np.float32(0.8221)]\n",
      "2025-04-22 06:40:57.340515: Epoch time: 351.42 s\n",
      "2025-04-22 06:40:57.340598: Yayy! New best EMA pseudo Dice: 0.8202999830245972\n",
      "2025-04-22 06:40:58.172220: \n",
      "2025-04-22 06:40:58.172475: Epoch 89\n",
      "2025-04-22 06:40:58.172558: Current learning rate: 0.00137\n",
      "2025-04-22 06:46:49.474480: train_loss -0.9341\n",
      "2025-04-22 06:46:49.475404: val_loss -0.7188\n",
      "2025-04-22 06:46:49.475521: Pseudo dice [np.float32(0.8239)]\n",
      "2025-04-22 06:46:49.475661: Epoch time: 351.3 s\n",
      "2025-04-22 06:46:49.475743: Yayy! New best EMA pseudo Dice: 0.8205999732017517\n",
      "2025-04-22 06:46:50.271658: \n",
      "2025-04-22 06:46:50.271942: Epoch 90\n",
      "2025-04-22 06:46:50.272048: Current learning rate: 0.00126\n",
      "2025-04-22 06:52:41.447041: train_loss -0.9346\n",
      "2025-04-22 06:52:41.449719: val_loss -0.7183\n",
      "2025-04-22 06:52:41.449831: Pseudo dice [np.float32(0.8225)]\n",
      "2025-04-22 06:52:41.450097: Epoch time: 351.18 s\n",
      "2025-04-22 06:52:41.450176: Yayy! New best EMA pseudo Dice: 0.8208000063896179\n",
      "2025-04-22 06:52:42.455277: \n",
      "2025-04-22 06:52:42.455540: Epoch 91\n",
      "2025-04-22 06:52:42.455632: Current learning rate: 0.00115\n",
      "2025-04-22 06:58:33.478534: train_loss -0.9394\n",
      "2025-04-22 06:58:33.481079: val_loss -0.7161\n",
      "2025-04-22 06:58:33.481214: Pseudo dice [np.float32(0.8202)]\n",
      "2025-04-22 06:58:33.481491: Epoch time: 351.02 s\n",
      "2025-04-22 06:58:34.380503: \n",
      "2025-04-22 06:58:34.380622: Epoch 92\n",
      "2025-04-22 06:58:34.380696: Current learning rate: 0.00103\n",
      "2025-04-22 07:04:26.193717: train_loss -0.9361\n",
      "2025-04-22 07:04:26.195726: val_loss -0.7144\n",
      "2025-04-22 07:04:26.195829: Pseudo dice [np.float32(0.8198)]\n",
      "2025-04-22 07:04:26.196044: Epoch time: 351.81 s\n",
      "2025-04-22 07:04:26.599452: \n",
      "2025-04-22 07:04:26.599577: Epoch 93\n",
      "2025-04-22 07:04:26.599660: Current learning rate: 0.00091\n",
      "2025-04-22 07:10:19.753726: train_loss -0.9385\n",
      "2025-04-22 07:10:19.756129: val_loss -0.7181\n",
      "2025-04-22 07:10:19.756243: Pseudo dice [np.float32(0.8222)]\n",
      "2025-04-22 07:10:19.756553: Epoch time: 353.15 s\n",
      "2025-04-22 07:10:20.162182: \n",
      "2025-04-22 07:10:20.162281: Epoch 94\n",
      "2025-04-22 07:10:20.162351: Current learning rate: 0.00079\n",
      "2025-04-22 07:16:12.718602: train_loss -0.9384\n",
      "2025-04-22 07:16:12.721025: val_loss -0.7209\n",
      "2025-04-22 07:16:12.721148: Pseudo dice [np.float32(0.8203)]\n",
      "2025-04-22 07:16:12.721469: Epoch time: 352.56 s\n",
      "2025-04-22 07:16:13.107406: \n",
      "2025-04-22 07:16:13.107550: Epoch 95\n",
      "2025-04-22 07:16:13.107629: Current learning rate: 0.00067\n",
      "2025-04-22 07:22:04.464825: train_loss -0.9373\n",
      "2025-04-22 07:22:04.467082: val_loss -0.7134\n",
      "2025-04-22 07:22:04.467211: Pseudo dice [np.float32(0.8202)]\n",
      "2025-04-22 07:22:04.467553: Epoch time: 351.36 s\n",
      "2025-04-22 07:22:04.901794: \n",
      "2025-04-22 07:22:04.901922: Epoch 96\n",
      "2025-04-22 07:22:04.902002: Current learning rate: 0.00055\n",
      "2025-04-22 07:27:55.841689: train_loss -0.9371\n",
      "2025-04-22 07:27:55.844356: val_loss -0.7214\n",
      "2025-04-22 07:27:55.844479: Pseudo dice [np.float32(0.8241)]\n",
      "2025-04-22 07:27:55.844732: Epoch time: 350.94 s\n",
      "2025-04-22 07:27:55.844820: Yayy! New best EMA pseudo Dice: 0.8209999799728394\n",
      "2025-04-22 07:27:56.899315: \n",
      "2025-04-22 07:27:56.899680: Epoch 97\n",
      "2025-04-22 07:27:56.899932: Current learning rate: 0.00043\n",
      "2025-04-22 07:33:47.575121: train_loss -0.9361\n",
      "2025-04-22 07:33:47.577870: val_loss -0.711\n",
      "2025-04-22 07:33:47.577992: Pseudo dice [np.float32(0.8181)]\n",
      "2025-04-22 07:33:47.578238: Epoch time: 350.68 s\n",
      "2025-04-22 07:33:48.017613: \n",
      "2025-04-22 07:33:48.017730: Epoch 98\n",
      "2025-04-22 07:33:48.017813: Current learning rate: 0.0003\n",
      "2025-04-22 07:39:36.671934: train_loss -0.9376\n",
      "2025-04-22 07:39:36.673988: val_loss -0.7093\n",
      "2025-04-22 07:39:36.674109: Pseudo dice [np.float32(0.8171)]\n",
      "2025-04-22 07:39:36.674493: Epoch time: 348.66 s\n",
      "2025-04-22 07:39:37.645406: \n",
      "2025-04-22 07:39:37.645523: Epoch 99\n",
      "2025-04-22 07:39:37.645599: Current learning rate: 0.00016\n",
      "2025-04-22 07:45:26.820505: train_loss -0.939\n",
      "2025-04-22 07:45:26.821792: val_loss -0.7157\n",
      "2025-04-22 07:45:26.821918: Pseudo dice [np.float32(0.8216)]\n",
      "2025-04-22 07:45:26.822073: Epoch time: 349.18 s\n",
      "2025-04-22 07:45:28.362302: Training done.\n",
      "perform_everything_on_device=True is only supported for cuda devices! Setting this to False\n",
      "2025-04-22 07:45:28.395898: Using splits from existing split file: /Users/yuma/Yuma-Kanematsu/nnUNet/src/preprocessed_data/Dataset005_Orbital_ROI/splits_final.json\n",
      "2025-04-22 07:45:28.396740: The split file contains 5 splits.\n",
      "2025-04-22 07:45:28.396833: Desired fold for training: 0\n",
      "2025-04-22 07:45:28.396881: This split has 165 training and 40 validation cases.\n",
      "2025-04-22 07:45:28.397050: predicting 6951166_19_R_MED-2\n",
      "2025-04-22 07:45:28.403449: 6951166_19_R_MED-2, shape torch.Size([1, 1, 324, 512]), rank 0\n",
      "2025-04-22 07:45:29.452887: predicting 6951166_20_R_MED-3\n",
      "2025-04-22 07:45:29.460826: 6951166_20_R_MED-3, shape torch.Size([1, 1, 304, 512]), rank 0\n",
      "2025-04-22 07:45:29.618434: predicting 6951166_21_R_MED-3\n",
      "2025-04-22 07:45:29.622747: 6951166_21_R_MED-3, shape torch.Size([1, 1, 301, 512]), rank 0\n",
      "2025-04-22 07:45:29.774228: predicting 6951166_22_R_MED-3\n",
      "2025-04-22 07:45:29.779151: 6951166_22_R_MED-3, shape torch.Size([1, 1, 313, 512]), rank 0\n",
      "2025-04-22 07:45:29.931911: predicting 6951166_23_R_MED-3\n",
      "2025-04-22 07:45:29.934691: 6951166_23_R_MED-3, shape torch.Size([1, 1, 343, 512]), rank 0\n",
      "2025-04-22 07:45:30.079120: predicting 6951166_24_R_MED-3\n",
      "2025-04-22 07:45:30.081256: 6951166_24_R_MED-3, shape torch.Size([1, 1, 341, 512]), rank 0\n",
      "2025-04-22 07:45:30.230515: predicting 7024296_18_L_FLOOR-3\n",
      "2025-04-22 07:45:30.233494: 7024296_18_L_FLOOR-3, shape torch.Size([1, 1, 352, 512]), rank 0\n",
      "2025-04-22 07:45:30.388441: predicting 7024296_19_L_FLOOR-3\n",
      "2025-04-22 07:45:30.391027: 7024296_19_L_FLOOR-3, shape torch.Size([1, 1, 387, 512]), rank 0\n",
      "2025-04-22 07:45:30.684281: predicting 7024296_20_L_FLOOR-3\n",
      "2025-04-22 07:45:30.686906: 7024296_20_L_FLOOR-3, shape torch.Size([1, 1, 398, 512]), rank 0\n",
      "2025-04-22 07:45:30.984137: predicting 7024296_21_L_FLOOR-3\n",
      "2025-04-22 07:45:30.995173: 7024296_21_L_FLOOR-3, shape torch.Size([1, 1, 406, 512]), rank 0\n",
      "2025-04-22 07:45:35.942281: predicting 7024296_22_L_FLOOR-3\n",
      "2025-04-22 07:45:35.946841: 7024296_22_L_FLOOR-3, shape torch.Size([1, 1, 420, 512]), rank 0\n",
      "2025-04-22 07:45:36.503315: predicting 7338791_15_R_FLOOR-3\n",
      "2025-04-22 07:45:36.508129: 7338791_15_R_FLOOR-3, shape torch.Size([1, 1, 353, 512]), rank 0\n",
      "2025-04-22 07:45:36.648781: predicting 7795253_26_R_FLOOR-3\n",
      "2025-04-22 07:45:36.653554: 7795253_26_R_FLOOR-3, shape torch.Size([1, 1, 357, 512]), rank 0\n",
      "2025-04-22 07:45:36.791785: predicting 7795253_27_R_FLOOR-3\n",
      "2025-04-22 07:45:36.795914: 7795253_27_R_FLOOR-3, shape torch.Size([1, 1, 359, 512]), rank 0\n",
      "2025-04-22 07:45:36.935578: predicting 7795253_28_R_FLOOR-3\n",
      "2025-04-22 07:45:36.939217: 7795253_28_R_FLOOR-3, shape torch.Size([1, 1, 361, 512]), rank 0\n",
      "2025-04-22 07:45:37.079418: predicting 7795253_29_R_FLOOR-3\n",
      "2025-04-22 07:45:37.083414: 7795253_29_R_FLOOR-3, shape torch.Size([1, 1, 363, 511]), rank 0\n",
      "2025-04-22 07:45:37.222105: predicting 7795253_30_R_FLOOR-3\n",
      "2025-04-22 07:45:37.226625: 7795253_30_R_FLOOR-3, shape torch.Size([1, 1, 367, 512]), rank 0\n",
      "2025-04-22 07:45:37.369537: predicting 7795253_31_R_FLOOR-3\n",
      "2025-04-22 07:45:37.373738: 7795253_31_R_FLOOR-3, shape torch.Size([1, 1, 368, 511]), rank 0\n",
      "2025-04-22 07:45:37.513430: predicting 7795253_32_R_FLOOR-3\n",
      "2025-04-22 07:45:37.518911: 7795253_32_R_FLOOR-3, shape torch.Size([1, 1, 370, 512]), rank 0\n",
      "2025-04-22 07:45:37.658656: predicting 7795253_33_R_FLOOR-3\n",
      "2025-04-22 07:45:37.663725: 7795253_33_R_FLOOR-3, shape torch.Size([1, 1, 375, 512]), rank 0\n",
      "2025-04-22 07:45:37.802355: predicting 7795253_34_R_FLOOR-3\n",
      "2025-04-22 07:45:37.807026: 7795253_34_R_FLOOR-3, shape torch.Size([1, 1, 375, 512]), rank 0\n",
      "2025-04-22 07:45:37.946365: predicting 7795253_35_R_FLOOR-3\n",
      "2025-04-22 07:45:37.951095: 7795253_35_R_FLOOR-3, shape torch.Size([1, 1, 376, 512]), rank 0\n",
      "2025-04-22 07:45:38.091277: predicting 7795253_37_R_FLOOR-3\n",
      "2025-04-22 07:45:38.095849: 7795253_37_R_FLOOR-3, shape torch.Size([1, 1, 373, 512]), rank 0\n",
      "2025-04-22 07:45:38.234732: predicting 7795253_38_R_FLOOR-3\n",
      "2025-04-22 07:45:38.239454: 7795253_38_R_FLOOR-3, shape torch.Size([1, 1, 383, 512]), rank 0\n",
      "2025-04-22 07:45:38.377539: predicting 7795253_39_R_FLOOR-3\n",
      "2025-04-22 07:45:38.382433: 7795253_39_R_FLOOR-3, shape torch.Size([1, 1, 391, 512]), rank 0\n",
      "2025-04-22 07:45:38.657893: predicting 7795253_40_R_FLOOR-3\n",
      "2025-04-22 07:45:38.663064: 7795253_40_R_FLOOR-3, shape torch.Size([1, 1, 403, 512]), rank 0\n",
      "2025-04-22 07:45:38.944667: predicting 7795253_42_R_FLOOR-3\n",
      "2025-04-22 07:45:38.949295: 7795253_42_R_FLOOR-3, shape torch.Size([1, 1, 415, 512]), rank 0\n",
      "2025-04-22 07:45:39.225902: predicting 7795253_43_R_FLOOR-3\n",
      "2025-04-22 07:45:39.229857: 7795253_43_R_FLOOR-3, shape torch.Size([1, 1, 421, 511]), rank 0\n",
      "2025-04-22 07:45:39.504255: predicting 7795253_44_R_FLOOR-3\n",
      "2025-04-22 07:45:39.509452: 7795253_44_R_FLOOR-3, shape torch.Size([1, 1, 428, 512]), rank 0\n",
      "2025-04-22 07:45:39.780222: predicting 7795253_45_R_FLOOR-3\n",
      "2025-04-22 07:45:39.785312: 7795253_45_R_FLOOR-3, shape torch.Size([1, 1, 428, 512]), rank 0\n",
      "2025-04-22 07:45:40.060994: predicting 8693888_11_L_MED-2\n",
      "2025-04-22 07:45:40.066323: 8693888_11_L_MED-2, shape torch.Size([1, 1, 350, 511]), rank 0\n",
      "2025-04-22 07:45:40.206166: predicting 8693888_12_L_MED-3\n",
      "2025-04-22 07:45:40.211513: 8693888_12_L_MED-3, shape torch.Size([1, 1, 379, 512]), rank 0\n",
      "2025-04-22 07:45:40.350140: predicting 8693888_13_L_MED-3\n",
      "2025-04-22 07:45:40.353530: 8693888_13_L_MED-3, shape torch.Size([1, 1, 381, 512]), rank 0\n",
      "2025-04-22 07:45:40.492674: predicting 8693888_14_L_MED-3\n",
      "2025-04-22 07:45:40.497840: 8693888_14_L_MED-3, shape torch.Size([1, 1, 381, 512]), rank 0\n",
      "2025-04-22 07:45:40.637624: predicting 8693888_15_L_MED-3\n",
      "2025-04-22 07:45:40.642031: 8693888_15_L_MED-3, shape torch.Size([1, 1, 369, 512]), rank 0\n",
      "2025-04-22 07:45:40.781155: predicting 8693888_16_L_MED-3\n",
      "2025-04-22 07:45:40.785479: 8693888_16_L_MED-3, shape torch.Size([1, 1, 364, 512]), rank 0\n",
      "2025-04-22 07:45:40.925924: predicting 8693888_17_L_MED-3\n",
      "2025-04-22 07:45:40.930337: 8693888_17_L_MED-3, shape torch.Size([1, 1, 349, 512]), rank 0\n",
      "2025-04-22 07:45:41.069942: predicting 8693888_18_L_MED-3\n",
      "2025-04-22 07:45:41.074111: 8693888_18_L_MED-3, shape torch.Size([1, 1, 354, 512]), rank 0\n",
      "2025-04-22 07:45:41.216632: predicting 8693888_19_L_MED-3\n",
      "2025-04-22 07:45:41.220979: 8693888_19_L_MED-3, shape torch.Size([1, 1, 365, 511]), rank 0\n",
      "2025-04-22 07:45:41.360791: predicting 8693888_20_L_MED-3\n",
      "2025-04-22 07:45:41.364947: 8693888_20_L_MED-3, shape torch.Size([1, 1, 384, 512]), rank 0\n",
      "2025-04-22 07:45:44.796782: Validation complete\n",
      "2025-04-22 07:45:44.796934: Mean Validation Dice:  0.8042954881354685\n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_train 005 2d 0 -device mps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
